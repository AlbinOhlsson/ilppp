<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC
        "-//W3C//DTD XHTML 1.0 Strict//EN"
        "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xml:lang="en" xmlns="http://www.w3.org/1999/xhtml">
    <head>
        <title>Assignment #3: Extracting noun groups using machine learning techniques. Java version</title>
    </head>
    <body>
        <!--<h1>Assignment #3: Extracting noun groups using machine learning techniques. Java version</h1>-->
        <h2>Objectives</h2>
        <p>The objectives of this assignment are to:</p>
        <ul>
            <li>Write a program to detect partial syntactic structures</li>
            <li>Understand the principles of supervised machine learning techniques applied to language processing</li>
            <li>Use a popular machine learning toolkit</li>
        </ul>
        <h2>Organization and location</h2>
        <p>The third lab session will take place on</p>
        <ul>
            <li>Group 1: Wednesday, September 23 from 10:15 to 12:00 in the Alpha room</li>
            <li>Group 2: Wednesday, September 23 from 10:15 to 12:00 in the Beta room</li>
            <li>Group 3: Wednesday, September 23 from 13:15 to 15:00 in the Beta room</li>
        </ul>
        <p>You can work alone or collaborate with another student.</p>
        <p>Each group will have to:</p>
        <ul>
            <li>Write and train a machine learning program to detect noun groups.</li>
            <li>Evaluate the results and comment them briefly.</li>
        </ul>
        <h2>Programming</h2>
        <h3>Choosing a training and a test sets</h3>
        <ol>
            <li>As annotated data and annotation scheme, you will use the data available from <a
                    href="http://www.cnts.ua.ac.be/conll2000/chunking/">CoNLL 2000</a>.
            </li>
            <li>Download both the training and test sets (the same as in
                the previous assignment) and decompress them.
                <br/>
                (Local copies available here: [<a
                        href="http://fileadmin.cs.lth.se/cs/Education/EDAN20/corpus/conll2000/train.txt">train.txt</a>]
                [<a href="http://fileadmin.cs.lth.se/cs/Education/EDAN20/corpus/conll2000/test.txt">test.txt</a>].)
            </li>
            <li>Be sure that you have a running
                <a
                        href="http://www.cs.waikato.ac.nz/ml/weka/">Weka
                </a>
                system:
                start it by typing <tt>weka</tt>. You must use version 3.7.10 or higher. You can increase the memory by
                typing<tt>java -Xmx2g -jar weka.jar</tt>, where the
                <tt>mx</tt>
                parameter corresponds to the maximum heap size.
            </li>
            <li>As a refresher on Java maps, i.e. name/value structures or dictionaries,
                read this program and run it on the Selma corpus [<a
                        href="http://fileadmin.cs.lth.se/cs/Education/EDAN20/programs/chunker/src/ora/WordSort.java">
                    link to the code</a>].
                <br/>
                The code is taken from Patrick Niemeyer and Daniel Leuck, Learning Java, 4th edition, 2013, O'Reilly,
                page 392. [<a href="http://shop.oreilly.com/product/0636920023463.do">link to the book</a>].
            </li>
        </ol>
        <h3>Baseline</h3>
        <p>Most statistical algorithms for language processing start with a so-called baseline. The baseline figure
            corresponds to the application of a minimal technique that is used to assess the difficulty of a task and
            for comparison with further programs.
        </p>
        <ol>
            <li>Read the baseline proposed by the organizers of the
                <a href="http://www.cnts.ua.ac.be/conll2000/chunking/">CoNLL 2000 shared task</a>
                (In the
                <i>Results</i>
                Sect.). What do you think of it?
            </li>
            <li>Implement this baseline program. You may either create a completely new program or start from an
                existing program that you will modify [<a
                        href="https://github.com/pnugues/ilppp/tree/master/programs/labs/chunking/chunker_java_ml/src">
                    Program folder
                </a>]. In this case:
                <ol>
                    <li>Modify and use the
                        <tt>BLChunkerIncomplete</tt>
                        program
                        to read the training set of CoNLL 2000 and collect the most frequent chunk tag for each
                        part-of-speech tag. You will have to complement the
                        <tt>countAssoc()</tt>
                        and
                        <tt>selectBestAssoc()</tt>
                        methods. You will also have to modify the paths in
                        <tt>Constants.java</tt>
                        so that they match your folder organization.
                    </li>
                    <li>Complement the
                        <tt>tag()</tt>
                        method to assign a part-of-speech value its most frequent chunk tag.
                        <br/>
                        Apply it to the test set and save the result in a result file.
                    </li>
                </ol>
            </li>

            <li>Measure the performance of the system. Use the
                <a href="https://github.com/pnugues/ilppp/tree/master/programs/corpus/conll2000">
                    <tt>eval.sh</tt>
                </a>
                script and the
                <a href="http://www.cnts.ua.ac.be/conll2000/chunking/">
                    <tt>conlleval.txt</tt>
                </a>
                evaluation program used by the CoNLL 2000 shared task.
            </li>
        </ol>
        <h3>Replicating the baseline with decision trees</h3>
        <p>In this exercise, you will replicate the baseline experiment with a popular machine-learning toolkit: <a
                href="http://www.cs.waikato.ac.nz/ml/weka/">Weka</a>. Weka is written in Java and comes with many
            ready-to-use algorithms.
        </p>
        <p>You will not write programs, but merely understand the overall process involving a machine-learning toolkit.
            You will train and apply models using the Java code supplied in the previous section.
        </p>
        <ol>
            <li>Weka uses a parameter file to learn a model. The parameter data are described by a header in the form of
                attributes and values. Download the
                <a href="https://github.com/pnugues/ilppp/tree/master/programs/corpus/conll2000">chunk_header.arff

                </a> header
                and modify it to fit the baseline experiment. Do you need the word attribute to replicate the baseline?
                <br/>
                The header is hard-coded in the Java program.
            </li>
            <li>Read and run the
                <tt>MLChunker</tt>
                class from this [<a
                        href="https://github.com/pnugues/ilppp/tree/master/programs/labs/chunking/chunker_java_ml/src">
                    program folder</a>] to
                create a Weka-compatible training file from the training set:
                <ul>
                    <li>Reuse the Java program to load the training file and read the lines.
                    </li>
                    <li>The Java program extracts two arguments: The part of speech
                        and the chunk type from each
                        line and write them in an ARFF file. The fields are separated by a
                        comma.
                    </li>
                </ul>
            </li>
            <li>From your ARFF training file, learn a decision tree using Weka and produce a model.
                <ul>
                    <li>Load your ARFF file by selecting the
                        <i>Preprocess button</i>
                    </li>
                    <li>Choose and create a classifier by pressing the
                        <i>Classify</i>
                        button and then the
                        <i>Choose</i>
                        button. Use the
                        <b>J48</b>
                        decision trees.
                    </li>
                    <li>Save the model by right-clicking on the item in the
                        <i>Result list</i>
                    </li>
                </ul>
            </li>
            <li>Embed your model in the Java program and measure its efficiency.
                <ul>
                    <li>Read the
                        <tt>tag()</tt>
                        method in
                        <tt>MLChunker.java</tt>
                        and
                        <tt>WekaGlue.java</tt>
                        to have Weka predict the chunk tag of your feature vector.
                    </li>
                    <li>Add the
                        <tt>weka.jar</tt>
                        library to your program. You will find it in the
                        <tt>/usr/local/cs/EDAN20</tt>
                        folder or you can download it from the Weka site. (Use version 3.7.10 or higher).
                    </li>
                    <li>Run
                        <tt>MLChunker</tt>
                        to read the words and parts of speech of each line of the test set and predict the chunk tag.
                        The input to this predicate is the list of (Word, POS) pairs. The predicate will determine the
                        chunk type and write the results in the CoNLL format: Word POS Chunk.
                    </li>
                    <li>Evaluate the results using the evaluation script.</li>
                </ul>
            </li>
        </ol>
        <h3>Using more features
        </h3>
        <p>In this exercise, you will extend the
            <tt>MLChunker</tt>
            program using more features. You will start from the Java program you downloaded and add one or two more
            features so that you improve the performance of your chunker.
        </p>
        <p>The program that won the CoNLL 2000 shared task (Kudoh and Matsumoto, 2000) used a window of five words
            around the chunk tag to identify,
            <i>c
                <sub>i</sub>
            </i>
            . They built a feature vector consisting of:
        </p>
        <ul>
            <li>The values of the five words in this window:
                <i>w
                    <sub>i-2</sub>
                </i>
                ,
                <i>w
                    <sub>i-1</sub>
                </i>
                ,
                <i>w
                    <sub>i</sub>
                </i>
                ,
                <i>w
                    <sub>i+1</sub>
                </i>
                ,
                <i>w
                    <sub>i+2</sub>
                </i>
            </li>
            <li>The values of the five parts of speech in this window:
                <i>t
                    <sub>i-2</sub>
                </i>
                ,
                <i>t
                    <sub>i-1</sub>
                </i>
                ,
                <i>t
                    <sub>i</sub>
                </i>
                ,
                <i>t
                    <sub>i+1</sub>
                </i>
                ,
                <i>t
                    <sub>i+2</sub>
                </i>
            </li>
            <li>The values of the two previous chunk tags in the first part of the window:
                <i>c
                    <sub>i-2</sub>
                </i>
                ,
                <i>c
                    <sub>i-1</sub>
                </i>
            </li>
        </ul>
        <p>The two last parameters are said to be dynamic because the program computes them at run-time. Kudoh and
            Matsumoto trained a classifier based on support vector machines. Read
            <a href="http://www.cnts.ua.ac.be/conll2000/pdf/14244kud.pdf">Kudoh and Matsumoto's paper</a>
            and the
            <a href="http://www.chasen.org/~taku/software/yamcha/">Yamcha</a>
            software site.
        </p>
        <ol>
            <li>What is the feature vector that corresponds to the baseline?</li>
            <li>Extract a set of features from the training set to improve the baseline. Use for instance (
                <i>t
                    <sub>i-1</sub>
                </i>
                ,
                <i>t
                    <sub>i</sub>
                </i>
                ) or better, (
                <i>t
                    <sub>i-1</sub>
                </i>
                ,
                <i>t
                    <sub>i</sub>
                </i>
                ,
                <i>t
                    <sub>i+1</sub>
                </i>
                ). In the beginning and end of the sentence, the feature vector will overflow as there is no
                <i>t
                    <sub>-1</sub>
                </i>
                . Use padding codes such as BOS and EOS (beginning of sentence, end of sentence)
            </li>
            <li>Train and embed your classifier in your Java program and measure its efficiency on the test set. You
                need to reach a global accuracy F1 score of 81 to pass this laboratory.
            </li>
        </ol>
    </body>
</html>
